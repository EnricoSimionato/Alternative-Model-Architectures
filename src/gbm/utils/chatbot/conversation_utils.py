from typing import Optional

import torch

from transformers import (
    AutoModelForCausalLM,
    PreTrainedTokenizer,
    StoppingCriteria
)


def get_conversation_example_1(
) -> list[str]:
    """
    Retrieves an example conversation for testing the abilities to interact of the model and its knowledge.

    Returns:
        list[str]:
            Example conversation.
    """
    example = ["Hi!",
               "Is Rome the capital of Italy?",
               "Is Paris the capital of France?",
               "Is Barcelona the capital of Spain?",
               "Is Madrid the capital of Russia?",
              ]

    return example


def get_conversation_example_2(
) -> list[str]:
    """
    Retrieves an example conversation for testing the abilities to interact of the model and its memory and reasoning.

    Returns:
        list[str]:
            Example conversation.
    """
    example = ["Hi!",
               "My name is Enrico and I am 24 years old",
               "Football is my favourite sport",
               "Can you tell me my age one year from now?",
               "What is my favourite sport?",
              ]

    return example


class StoppingCriteriaCallback(StoppingCriteria):
    """
    Callback to stop the generation of a language model when calling its "generate" method.

    Args:
        tokenizer (transformers.PreTrainedTokenizer):
            The tokenizer object used for encoding and decoding.
        end_tokens (list[str]):
            A list of end tokens indicating when to stop generation.
        *args:
            Additional arguments to pass to the superclass constructor.
        **kwargs:
            Additional keyword arguments to pass to the superclass constructor.

    Attributes:
        tokenizer (transformers.PreTrainedTokenizer):
            The tokenizer object used for encoding and decoding.
        end_tokens ():
            A list of end tokens indicating when to stop generation.
    """

    def __init__(
            self,
            tokenizer: PreTrainedTokenizer,
            end_tokens: list[str],
            *args,
            **kwargs
    ) -> None:

        super().__init__(*args, **kwargs)
        self.tokenizer = tokenizer
        self.end_tokens = end_tokens

    def __call__(
            self,
            token_ids: torch.LongTensor,
            scores: torch.FloatTensor,
            **kwargs
    ) -> bool:
        """
        Determines whether to stop generation based on the presence of end tokens.

        Args:
            token_ids (torch.Tensor):
                The token IDs generated by the model.
            scores (torch.Tensor):
                The scores associated with the generated input IDs.
            **kwargs:
                Additional keyword arguments.

        Returns:
            bool:
                True if generation should stop, False otherwise.
        """

        decoded_tokens = self.tokenizer.decode(token_ids.squeeze())

        for end_token in self.end_tokens:
            if decoded_tokens.endswith(end_token):
                #print(f"[ DEBUG ] Generation stopped on {end_token} [ DEBUG ]")
                return True

        return False


def start_conversation_loop(
        model: AutoModelForCausalLM,
        tokenizer: PreTrainedTokenizer,
        stop_tokens: list[str] = ("[INST]", "</s>"),
        remove_stop_tokens: list[bool] = (True, False),
        max_len: int = 5,
        user_inputs: Optional[list[str]] = None,
        print_conversation: bool = True,
        make_model_trainable: bool = True
) -> list[dict[str, str]]:
    """
    Allows to have a conversation with the trained model.

    Args:
        model (transformers.AutoModelForCausalLM):
            The trained model.
        tokenizer (transformers.PreTrainedTokenizer):
            Tokenizer object to encode the inputs.
        stop_tokens (list[str]):
            Tokens that, when given as output, stop the model generation of the
            model.
        remove_stop_tokens (list[bool]):
            Flags to remove the stop tokens from the chatbot response.
        max_len (int):
            Maximum length of the conversation. Defaults to 5. If max_len and
            user_inputs are both given, max_len is ignored.
        user_inputs (Optional[list[str]]):
            List of user inputs. If provided, max_len is ignored. Defaults to
            None.
        print_conversation (bool):
            Whether to print the conversation. Defaults to True.
        make_model_trainable (bool):
            Whether to set the model back to train mode after the conversation
            loop. Defaults to True.

    Returns:
        dialogue (str):
            Complete dialogue including user inputs and chatbot responses.
    """

    if len(stop_tokens) != len(remove_stop_tokens):
        raise Exception(
            "The flags about the deletion of the stop tokens from the chatbot response has to be set for only and all "
            "the stop tokens!")

    if user_inputs is not None:
        max_len = len(user_inputs)

    model.eval()

    dialogue = []

    custom_stopping_criteria = StoppingCriteriaCallback(
        tokenizer,
        stop_tokens
    )

    for i in range(max_len):
        # Reading user message
        user_message = ""

        if user_inputs is None:
            user_message = input("User: ")
        else:
            user_message = user_inputs[i]
            if print_conversation:
                print(f"User: {user_inputs[i]}")
        print()

        dialogue.append({
            "role": "user",
            "content": user_message.strip()
        })

        # Encoding input and move it where the model is
        encoded_dialogue = tokenizer.apply_chat_template(dialogue, return_tensors="pt")

        # Generating model response
        encoded_dialogue = encoded_dialogue.to(model.device)
        output_ids = model.generate(encoded_dialogue,
                                    max_new_tokens=128,
                                    do_sample=True,
                                    top_p=0.9,
                                    top_k=0,
                                    pad_token_id=tokenizer.pad_token_id,
                                    stopping_criteria=[custom_stopping_criteria]
                                    )

        chatbot_response = tokenizer.decode(
            output_ids[0, encoded_dialogue.size(1):],
            skip_special_tokens=False
        )

        stop_token = [el for el in stop_tokens if chatbot_response.endswith(el)]
        if len(stop_token) > 0 and remove_stop_tokens[stop_tokens.index(stop_token[0])]:
            chatbot_response = chatbot_response.rstrip(
                stop_token[0]
            )

        # Appending chatbot response to dialogue history
        dialogue.append({
            "role": "assistant",
            "content": chatbot_response.strip()
        })

        # Printing chatbot response
        if print_conversation:
            print(f"Chatbot: {chatbot_response}")
            print()

    if make_model_trainable:
        model.train()

    return dialogue
