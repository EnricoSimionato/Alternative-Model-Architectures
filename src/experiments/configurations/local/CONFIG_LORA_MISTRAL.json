{
    "path_to_storage": "/Users/enricosimionato/Desktop/Alternative-Model-Architectures/src/experiments/performed_experiments",
    "keys_for_naming": [
        "adapter_method",
        "original_model_id"
    ],

    "original_model_id": "google/gemma-2b",
    "quantization": "8bit",

    "dataset_id": "timdettmers/openassistant-guanaco",
    "max_len_samples": 16384,
    "split": [
        0.8,
        0.1,
        0.1
    ],
    "tokenizer_id": "google/gemma-2b-it",
    "tokenizer_id_for_chat_template": "mistralai/Mistral-7B-Instruct-v0.2",
    "stop_tokens": [
        "[INST]",
        "</s>"
    ],
    "max_len_tokenizer": 512,
    "max_epochs": 1,
    "learning_rate": 1e-05,
    "warmup_steps": 0,
    "batch_size": 16,
    "num_workers": 2,
    "num_checks_per_epoch": 2,
    "gradient_accumulation_steps": 4,

    "verbose": 1,

    "adapter_method": "LoRA",
    "lora_alpha": 32,
    "target_modules": [
        "q_proj",
        "v_proj"
    ],
    "lora_dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM",

    "original_model_parameters": 0,
    "model_parameters": 0,
    "percentage_parameters": 0,
    "model_trainable_parameters": 0,

    "device": "cuda",
    "seed": 42
}
