{
    "path_to_storage": "/Users/enricosimionato/Desktop/Alternative-Model-Architectures/src/experiments/performed_experiments/analysis",
    
    "experiment_type": "benchmark_evaluation",

    "version": "0",
    
    "model_id": "/Users/enricosimionato/Desktop/Alternative-Model-Architectures/src/experiments/models/gpt-neo-1.3B",
    "quantization": null,
    "dtype": "float32",

    "factorization_method": "Hadamard",
    "target_layers": {
        "q_proj": {
            "rank": 256
        },
        "k_proj": {
            "rank": 256
        },
        "v_proj": {
            "rank": 256
        },
        "out_proj": {
            "rank": 256
        },
        "c_fc": {
            "rank": 256
        },
        "c_proj": {
            "rank": 256
        }
    },
    "original_model_parameters": 0,
    "model_parameters": 0,
    "percentage_parameters": 0,
    "model_trainable_parameters": 0,
    
    "tokenizer_id": "EleutherAI/gpt-neo-1.3B",
    
    "benchmark_ids": [
        "hellaswag"
    ],
    "evaluation_args": {
        "truthfulqa_mc1": {
            "batch_size": 8,
            "limit": 1
        },
        "arc_challenge": {
            "batch_size": 8,
            "limit": 1
        },
        "hellaswag": {
            "batch_size": 8,
            "limit": 1
        }
    },
    
    "figure_size": [30, 30],
    
    "verbose": 1,
    "device": "mps",
    "seed": 42
}
