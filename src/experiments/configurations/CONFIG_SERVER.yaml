########################################    General configuration parameters    ########################################
path_to_storage: "src/experiments/results"

version: 10

verbose: 1
device: "cuda"
seed: 42

########################################          Parameters gemma-2-2b         ########################################
model_id: "google/gemma-2-2b"
dtype: "float16"
tokenizer_id: "google/gemma-2-2b"
num_layers: 26

########################################         Parameters Llama-3.1-8B        ########################################
#model_id: "meta-llama/Llama-3.1-8B"
#dtype: "float16"
#tokenizer_id: "meta-llama/Llama-3.1-8B"
#num_layers: 32

########################################            ABACO Experiment            ########################################
experiment_type: "abaco_experiment"
figure_size: [5, 10]

########################################            ABACO Experiment            ########################################
adapter_methods:
    - "abaco-lora"
initial_alpha: 1.0
horizon: 80000
alpha_strategy: "exponential"

adapted_layers:
    q_proj:
        rank: 144

lora_dropout: 0.05
bias: "none"
task_type: "CAUSAL_LM"

########################################          Benchmarks parameters         ########################################
benchmark_ids:
    - "gsm8k"
    - "hellaswag"
evaluation_args:
    gsm8k:
        batch_size: 8
    hellaswag:
        batch_size: 16

########################################     General Fine-Tuning Parameters     ########################################
task_id: "causallm"

max_epochs: 20
gradient_accumulation_steps: 2
val_check_interval: 5000

dataset_id: "openwebtext"

split:
    - 0.8
    - 0.1
    - 0.1
batch_size: 2
num_workers: 2
max_len: 512

optimizers_settings:
    - optimizer: "AdamW"
      learning_rate: 0.001
      lr_scheduler: "cosine_with_warmup"
      warmup_steps: 100
      monitored_metric: "loss"

fine-tuning_targets:
    - ["block_index", "self_attn", "q_proj"]
