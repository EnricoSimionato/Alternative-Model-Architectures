########################################    General configuration parameters    ########################################
path_to_storage: "src/experiments/results"

#experiment_type: "factorization_benchmark_evaluation"
experiment_type: "factorization_fine_tuning_experiment"

#experiment_type: "layer_replacement_fine_tuning_experiment"
#experiment_type: "layer_replacement_fine_tuning_entire_model_experiment"
#experiment_type: "layer_replacement_fine_tuning_adapter_on_targets_experiment"
#experiment_type: "layer_replacement_fine_tuning_adapter_on_entire_model_experiment"

#just_plot: True
version: 1

verbose: 1
device: "cuda"
seed: 42

########################################     Model configuration parameters     ########################################
dtype: "float16"
#quantization: 4bit
model_id: "mistralai/Mistral-7B-v0.3"
#model_id: "google/gemma-2-2b"

########################################   Tokenizer configuration parameters   ########################################
tokenizer_id: "mistralai/Mistral-7B-v0.3"
#tokenizer_id: "google/gemma-2-2b"

########################################        Visualization parameters        ########################################
figure_size: [5, 10]

########################################     Layers factorization parameters    ########################################
factorization_methods:
    - "LocalSVD"
#    - "Hadamard"
#    - "GlobalBase"
#    - "GlobalBaseAverageSVDInitialization"

target_layers:
#    q_proj:
#        rank: 256
#    k_proj:
#        rank: 256
#    v_proj:
#        rank: 256
    gate_proj:
        rank: 1024
    up_proj:
        rank: 1024
    down_proj:
        rank: 2048

use_names_as_keys: True

########################################      Layers replacement parameters     ########################################
replacement_methods:
    - "sharedaveragelayer"
num_layers: 32

targets:
    - ["block_index", "mlp", "gate_proj"]
    - ["block_index", "mlp", "up_proj"]
#    - ["block_index", "mlp", "down_proj"]

########################################          Benchmarks parameters         ########################################
benchmark_ids:
    - "hellaswag"
evaluation_args:
    truthfulqa_mc1:
        batch_size: 32
    arc_challenge:
        batch_size: 32
    hellaswag:
        batch_size: 16

########################################         Fine-tuning parameters         ########################################
task_id: "causallm"
dataset_id: "wikitext2"

num_checks_per_epoch: 2
gradient_accumulation_steps: 2
max_len: 256

optimizers_settings:
    - optimizer: "AdamW"
      learning_rate: 0.01
      lr_scheduler: "cosine_with_warmup"
      warmup_steps: 100
      monitored_metric: "loss"

max_epochs: 10

split:
    - 0.8
    - 0.1
    - 0.1
batch_size: 1
num_workers: 2

#fast_dev_run: True
